{"version":3,"sources":["logo.svg","utilities.js","App.js","reportWebVitals.js","index.js"],"names":["color","toTuple","y","x","drawPoint","ctx","r","beginPath","arc","Math","PI","fillStyle","fill","drawSegment","scale","ay","ax","by","bx","moveTo","lineTo","lineWidth","strokeStyle","stroke","App","webcamRef","useRef","canvasRef","runPosenet","a","posenet","inputResolution","width","height","net","setInterval","detect","current","video","readyState","videoWidth","videoHeight","estimateSinglePose","pose","console","log","drawCanvas","canvas","getContext","keypoints","minConfidence","i","length","keypoint","score","position","drawKeypoints","forEach","drawSkeleton","className","ref","style","marginLeft","marginRight","left","right","textAlign","zindex","reportWebVitals","onPerfEntry","Function","then","getCLS","getFID","getFCP","getLCP","getTTFB","ReactDOM","render","StrictMode","document","getElementById"],"mappings":"8UAAe,I,+CCmBTA,EAAQ,OA2Dd,SAASC,EAAT,GACE,MAAO,CADkB,EAARC,EAAQ,EAALC,GAIf,SAASC,EAAUC,EAAKH,EAAGC,EAAGG,EAAGN,GACtCK,EAAIE,YACJF,EAAIG,IAAIL,EAAGD,EAAGI,EAAG,EAAG,EAAIG,KAAKC,IAC7BL,EAAIM,UAAYX,EAChBK,EAAIO,OAMC,SAASC,EAAT,IAAyCb,EAAOc,EAAOT,GAAM,IAAD,mBAAtCU,EAAsC,KAAlCC,EAAkC,wBAA5BC,EAA4B,KAAxBC,EAAwB,KACjEb,EAAIE,YACJF,EAAIc,OAAOH,EAAKF,EAAOC,EAAKD,GAC5BT,EAAIe,OAAOF,EAAKJ,EAAOG,EAAKH,GAC5BT,EAAIgB,UA3EY,EA4EhBhB,EAAIiB,YAActB,EAClBK,EAAIkB,SCNSC,MApFf,WACE,IAAMC,EAAYC,iBAAO,MACnBC,EAAYD,iBAAO,MAGnBE,EAAU,uCAAG,4BAAAC,EAAA,sEACCC,IAAa,CAC7BC,gBAAiB,CAACC,MAAM,IAAKC,OAAO,KACpCnB,MAAM,KAHS,OACXoB,EADW,OAMjBC,aAAY,WACVC,EAAOF,KACN,KARc,2CAAH,qDAWVE,EAAM,uCAAG,WAAOF,GAAP,qBAAAL,EAAA,yDACoB,qBAAtBJ,EAAUY,SAAiD,OAAtBZ,EAAUY,SAAyD,IAArCZ,EAAUY,QAAQC,MAAMC,WADzF,wBAGLD,EAAQb,EAAUY,QAAQC,MAC1BE,EAAaf,EAAUY,QAAQC,MAAME,WACrCC,EAAchB,EAAUY,QAAQC,MAAMG,YAG5ChB,EAAUY,QAAQC,MAAMN,MAAQQ,EAChCf,EAAUY,QAAQC,MAAML,OAASQ,EATtB,SAYQP,EAAIQ,mBAAmBJ,GAZ/B,OAYLK,EAZK,OAaXC,QAAQC,IAAIF,GAEZG,EAAWH,EAAML,EAAOE,EAAYC,EAAad,GAftC,4CAAH,sDAmBNmB,EAAa,SAACH,EAAML,EAAOE,EAAYC,EAAaM,GACxD,IAAM1C,EAAM0C,EAAOV,QAAQW,WAAW,MACtCD,EAAOV,QAAQL,MAAQQ,EACvBO,EAAOV,QAAQJ,OAASQ,ED8ErB,SAAuBQ,EAAWC,EAAe7C,GACtD,IADuE,IAAZS,EAAW,uDAAH,EAC1DqC,EAAI,EAAGA,EAAIF,EAAUG,OAAQD,IAAK,CACzC,IAAME,EAAWJ,EAAUE,GAE3B,KAAIE,EAASC,MAAQJ,GAArB,CAHyC,MAOxBG,EAASE,SAC1BnD,EAAUC,EAR+B,EAOjCH,EACWY,EARsB,EAO9BX,EACmBW,EAAO,EAAGd,KCrFxCwD,CAAcb,EAAI,UAAe,GAAKtC,GDwDnC,SAAsB4C,EAAWC,EAAe7C,GAAiB,IAAZS,EAAW,uDAAH,EACxCgB,IACxBmB,EACAC,GAGgBO,SAAQ,SAACR,GACzBpC,EACEZ,EAAQgD,EAAU,GAAGM,UACrBtD,EAAQgD,EAAU,GAAGM,UACrBvD,EACAc,EACAT,MCnEFqD,CAAaf,EAAI,UAAe,GAAKtC,IAMvC,OAHAuB,IAIE,qBAAK+B,UAAU,MAAf,SACE,yBAAQA,UAAU,aAAlB,cACE,cAAC,IAAD,CACEC,IAAKnC,EACLoC,MAAO,CACLN,SAAS,WACTO,WAAW,OACXC,YAAY,OACZC,KAAK,EACLC,MAAM,EACNC,UAAU,SACVC,OAAO,EACPnC,MAAM,IACNC,OAAO,OAIX,wBACE2B,IAAKjC,EACLkC,MAAO,CACLN,SAAS,WACTO,WAAW,OACXC,YAAY,OACZC,KAAK,EACLC,MAAM,EACNC,UAAU,SACVC,OAAO,EACPnC,MAAM,IACNC,OAAO,aCxEJmC,EAZS,SAAAC,GAClBA,GAAeA,aAAuBC,UACxC,8BAAqBC,MAAK,YAAkD,IAA/CC,EAA8C,EAA9CA,OAAQC,EAAsC,EAAtCA,OAAQC,EAA8B,EAA9BA,OAAQC,EAAsB,EAAtBA,OAAQC,EAAc,EAAdA,QAC3DJ,EAAOH,GACPI,EAAOJ,GACPK,EAAOL,GACPM,EAAON,GACPO,EAAQP,OCDdQ,IAASC,OACP,cAAC,IAAMC,WAAP,UACE,cAAC,EAAD,MAEFC,SAASC,eAAe,SAM1Bb,M","file":"static/js/main.0d204fbc.chunk.js","sourcesContent":["export default __webpack_public_path__ + \"static/media/logo.6ce24c58.svg\";","/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * https://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport * as posenet from \"@tensorflow-models/posenet\";\nimport * as tf from \"@tensorflow/tfjs\";\n\nconst color = \"aqua\";\nconst boundingBoxColor = \"red\";\nconst lineWidth = 2;\n\nexport const tryResNetButtonName = \"tryResNetButton\";\nexport const tryResNetButtonText = \"[New] Try ResNet50\";\nconst tryResNetButtonTextCss = \"width:100%;text-decoration:underline;\";\nconst tryResNetButtonBackgroundCss = \"background:#e61d5f;\";\n\nfunction isAndroid() {\n  return /Android/i.test(navigator.userAgent);\n}\n\nfunction isiOS() {\n  return /iPhone|iPad|iPod/i.test(navigator.userAgent);\n}\n\nexport function isMobile() {\n  return isAndroid() || isiOS();\n}\n\nfunction setDatGuiPropertyCss(propertyText, liCssString, spanCssString = \"\") {\n  var spans = document.getElementsByClassName(\"property-name\");\n  for (var i = 0; i < spans.length; i++) {\n    var text = spans[i].textContent || spans[i].innerText;\n    if (text == propertyText) {\n      spans[i].parentNode.parentNode.style = liCssString;\n      if (spanCssString !== \"\") {\n        spans[i].style = spanCssString;\n      }\n    }\n  }\n}\n\nexport function updateTryResNetButtonDatGuiCss() {\n  setDatGuiPropertyCss(\n    tryResNetButtonText,\n    tryResNetButtonBackgroundCss,\n    tryResNetButtonTextCss\n  );\n}\n\n/**\n * Toggles between the loading UI and the main canvas UI.\n */\nexport function toggleLoadingUI(\n  showLoadingUI,\n  loadingDivId = \"loading\",\n  mainDivId = \"main\"\n) {\n  if (showLoadingUI) {\n    document.getElementById(loadingDivId).style.display = \"block\";\n    document.getElementById(mainDivId).style.display = \"none\";\n  } else {\n    document.getElementById(loadingDivId).style.display = \"none\";\n    document.getElementById(mainDivId).style.display = \"block\";\n  }\n}\n\nfunction toTuple({ y, x }) {\n  return [y, x];\n}\n\nexport function drawPoint(ctx, y, x, r, color) {\n  ctx.beginPath();\n  ctx.arc(x, y, r, 0, 2 * Math.PI);\n  ctx.fillStyle = color;\n  ctx.fill();\n}\n\n/**\n * Draws a line on a canvas, i.e. a joint\n */\nexport function drawSegment([ay, ax], [by, bx], color, scale, ctx) {\n  ctx.beginPath();\n  ctx.moveTo(ax * scale, ay * scale);\n  ctx.lineTo(bx * scale, by * scale);\n  ctx.lineWidth = lineWidth;\n  ctx.strokeStyle = color;\n  ctx.stroke();\n}\n\n/**\n * Draws a pose skeleton by looking up all adjacent keypoints/joints\n */\nexport function drawSkeleton(keypoints, minConfidence, ctx, scale = 1) {\n  const adjacentKeyPoints = posenet.getAdjacentKeyPoints(\n    keypoints,\n    minConfidence\n  );\n\n  adjacentKeyPoints.forEach((keypoints) => {\n    drawSegment(\n      toTuple(keypoints[0].position),\n      toTuple(keypoints[1].position),\n      color,\n      scale,\n      ctx\n    );\n  });\n}\n\n/**\n * Draw pose keypoints onto a canvas\n */\nexport function drawKeypoints(keypoints, minConfidence, ctx, scale = 1) {\n  for (let i = 0; i < keypoints.length; i++) {\n    const keypoint = keypoints[i];\n\n    if (keypoint.score < minConfidence) {\n      continue;\n    }\n\n    const { y, x } = keypoint.position;\n    drawPoint(ctx, y * scale, x * scale, 3, color);\n  }\n}\n\n/**\n * Draw the bounding box of a pose. For example, for a whole person standing\n * in an image, the bounding box will begin at the nose and extend to one of\n * ankles\n */\nexport function drawBoundingBox(keypoints, ctx) {\n  const boundingBox = posenet.getBoundingBox(keypoints);\n\n  ctx.rect(\n    boundingBox.minX,\n    boundingBox.minY,\n    boundingBox.maxX - boundingBox.minX,\n    boundingBox.maxY - boundingBox.minY\n  );\n\n  ctx.strokeStyle = boundingBoxColor;\n  ctx.stroke();\n}\n\n/**\n * Converts an arary of pixel data into an ImageData object\n */\nexport async function renderToCanvas(a, ctx) {\n  const [height, width] = a.shape;\n  const imageData = new ImageData(width, height);\n\n  const data = await a.data();\n\n  for (let i = 0; i < height * width; ++i) {\n    const j = i * 4;\n    const k = i * 3;\n\n    imageData.data[j + 0] = data[k + 0];\n    imageData.data[j + 1] = data[k + 1];\n    imageData.data[j + 2] = data[k + 2];\n    imageData.data[j + 3] = 255;\n  }\n\n  ctx.putImageData(imageData, 0, 0);\n}\n\n/**\n * Draw an image on a canvas\n */\nexport function renderImageToCanvas(image, size, canvas) {\n  canvas.width = size[0];\n  canvas.height = size[1];\n  const ctx = canvas.getContext(\"2d\");\n\n  ctx.drawImage(image, 0, 0);\n}\n\n/**\n * Draw heatmap values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's heatmap outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\nexport function drawHeatMapValues(heatMapValues, outputStride, canvas) {\n  const ctx = canvas.getContext(\"2d\");\n  const radius = 5;\n  const scaledValues = heatMapValues.mul(tf.scalar(outputStride, \"int32\"));\n\n  drawPoints(ctx, scaledValues, radius, color);\n}\n\n/**\n * Used by the drawHeatMapValues method to draw heatmap points on to\n * the canvas\n */\nfunction drawPoints(ctx, points, radius, color) {\n  const data = points.buffer().values;\n\n  for (let i = 0; i < data.length; i += 2) {\n    const pointY = data[i];\n    const pointX = data[i + 1];\n\n    if (pointX !== 0 && pointY !== 0) {\n      ctx.beginPath();\n      ctx.arc(pointX, pointY, radius, 0, 2 * Math.PI);\n      ctx.fillStyle = color;\n      ctx.fill();\n    }\n  }\n}\n\n/**\n * Draw offset vector values, one of the model outputs, on to the canvas\n * Read our blog post for a description of PoseNet's offset vector outputs\n * https://medium.com/tensorflow/real-time-human-pose-estimation-in-the-browser-with-tensorflow-js-7dd0bc881cd5\n */\n// export function drawOffsetVectors(\n//     heatMapValues, offsets, outputStride, scale = 1, ctx) {\n//   const offsetPoints =\n//       posenet.singlePose.getOffsetPoints(heatMapValues, outputStride, offsets);\n\n//   const heatmapData = heatMapValues.buffer().values;\n//   const offsetPointsData = offsetPoints.buffer().values;\n\n//   for (let i = 0; i < heatmapData.length; i += 2) {\n//     const heatmapY = heatmapData[i] * outputStride;\n//     const heatmapX = heatmapData[i + 1] * outputStride;\n//     const offsetPointY = offsetPointsData[i];\n//     const offsetPointX = offsetPointsData[i + 1];\n\n//     drawSegment(\n//         [heatmapY, heatmapX], [offsetPointY, offsetPointX], color, scale, ctx);\n//   }\n// }\n","import React, {useRef} from 'react';\nimport logo from './logo.svg';\nimport './App.css';\nimport * as tf from \"@tensorflow/tfjs\";\nimport * as posenet from \"@tensorflow-models/posenet\";\nimport Webcam from \"react-webcam\";\nimport { drawKeypoints, drawSkeleton } from \"./utilities\";\n\nfunction App() {\n  const webcamRef = useRef(null);\n  const canvasRef = useRef(null);\n\n  // Load Posenet\n  const runPosenet = async () => {\n    const net = await posenet.load({\n      inputResolution: {width:640, height:480},\n      scale:0.5,\n    });\n\n    setInterval(() => {\n      detect(net);\n    }, 100);\n  };\n\n  const detect = async (net) => {\n    if (typeof webcamRef.current !== \"undefined\" && webcamRef.current !== null && webcamRef.current.video.readyState===4) {\n      // Get video properties\n      const video = webcamRef.current.video;\n      const videoWidth = webcamRef.current.video.videoWidth;\n      const videoHeight = webcamRef.current.video.videoHeight;\n\n      // Set video width\n      webcamRef.current.video.width = videoWidth\n      webcamRef.current.video.height = videoHeight\n\n      // Make Detections\n      const pose = await net.estimateSinglePose(video);\n      console.log(pose);\n\n      drawCanvas(pose, video, videoWidth, videoHeight, canvasRef);\n    }\n  };\n\n  const drawCanvas = (pose, video, videoWidth, videoHeight, canvas) => {\n    const ctx = canvas.current.getContext(\"2d\");\n    canvas.current.width = videoWidth;\n    canvas.current.height = videoHeight;\n\n    drawKeypoints(pose[\"keypoints\"], 0.5, ctx);\n    drawSkeleton(pose[\"keypoints\"], 0.5, ctx);\n  };\n\n  runPosenet();\n\n\n  return (\n    <div className=\"App\">\n      <header className=\"App-header\">\"\n        <Webcam\n          ref={webcamRef}\n          style={{\n            position:\"absolute\",\n            marginLeft:\"auto\",\n            marginRight:\"auto\",\n            left:0,\n            right:0,\n            textAlign:\"center\",\n            zindex:9,\n            width:640,\n            height:480\n            }}\n        />\n\n        <canvas\n          ref={canvasRef}\n          style={{\n            position:\"absolute\",\n            marginLeft:\"auto\",\n            marginRight:\"auto\",\n            left:0,\n            right:0,\n            textAlign:\"center\",\n            zindex:9,\n            width:640,\n            height:480\n            }}\n        />\n      </header>\n    </div>\n  );\n}\n\nexport default App;\n","const reportWebVitals = onPerfEntry => {\n  if (onPerfEntry && onPerfEntry instanceof Function) {\n    import('web-vitals').then(({ getCLS, getFID, getFCP, getLCP, getTTFB }) => {\n      getCLS(onPerfEntry);\n      getFID(onPerfEntry);\n      getFCP(onPerfEntry);\n      getLCP(onPerfEntry);\n      getTTFB(onPerfEntry);\n    });\n  }\n};\n\nexport default reportWebVitals;\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './App';\nimport reportWebVitals from './reportWebVitals';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want to start measuring performance in your app, pass a function\n// to log results (for example: reportWebVitals(console.log))\n// or send to an analytics endpoint. Learn more: https://bit.ly/CRA-vitals\nreportWebVitals();\n"],"sourceRoot":""}